{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# US Immigration\n",
    "### Data Engineering Capstone Project\n",
    "#### Project Description\n",
    "\n",
    "* In this project we are going to design a database schema for easier processing and analysis of the US immigration data. As data engineer, we built a etl pipeline that extracts data from different sources, cleans the data and loads the data into respective tables for easier querying and analysing the data that impact the behaviour of the immigrants in US\n",
    "\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as f\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope of the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "* I94 Immigration, city data (both census and temperature), airport data are used to design a database schema that is optimized to easily query and analyze immigration events. An ETL pipeline is built with these data sources to create the star schema-based design. Finally, the database can be used to analyze immigration behavior to location data.\n",
    "\n",
    "\n",
    "#### Describe and Gather Data \n",
    "##### The following datasets are used to create the database schema. \n",
    "* I94 Immigration Data: This data comes from the US National Tourism and Trade Office and it contains the SAS7BDAT file for each month of the year. Each file has 3M rows with 28 columns each.\n",
    "* World Temperature Data: This dataset came from Kaggle and contains the temperature of the cities\n",
    "* U.S. City Demographic Data: This data comes from OpenSoft and data is about city demographics\n",
    "* Airport Code Table:  it consists of information about different airports around the world\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "fname = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "df = pd.read_sas(fname, 'sas7bdat', encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>10282016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.897628e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.736796e+09</td>\n",
       "      <td>00296</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20691.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OS</td>\n",
       "      <td>6.666432e+08</td>\n",
       "      <td>93</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0    6.0  2016.0     4.0   692.0   692.0     XXX  20573.0      NaN     NaN   \n",
       "1    7.0  2016.0     4.0   254.0   276.0     ATL  20551.0      1.0      AL   \n",
       "2   15.0  2016.0     4.0   101.0   101.0     WAS  20545.0      1.0      MI   \n",
       "3   16.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "4   17.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "\n",
       "   depdate   ...     entdepu  matflag  biryear   dtaddto gender insnum  \\\n",
       "0      NaN   ...           U      NaN   1979.0  10282016    NaN    NaN   \n",
       "1      NaN   ...           Y      NaN   1991.0       D/S      M    NaN   \n",
       "2  20691.0   ...         NaN        M   1961.0  09302016      M    NaN   \n",
       "3  20567.0   ...         NaN        M   1988.0  09302016    NaN    NaN   \n",
       "4  20567.0   ...         NaN        M   2012.0  09302016    NaN    NaN   \n",
       "\n",
       "  airline        admnum  fltno visatype  \n",
       "0     NaN  1.897628e+09    NaN       B2  \n",
       "1     NaN  3.736796e+09  00296       F1  \n",
       "2      OS  6.666432e+08     93       B2  \n",
       "3      AA  9.246846e+10  00199       B2  \n",
       "4      AA  9.246846e+10  00199       B2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()\n",
    "df_spark =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#gathering required data\n",
    "#df_spark.write.parquet(\"sas_data\")\n",
    "df_I94immigration = spark.read.parquet(\"sas_data\")\n",
    "df_airport = spark.read.csv('airport-codes_csv.csv', header=True, inferSchema=True)\n",
    "df_usdemo = spark.read.csv('us-cities-demographics.csv', header = True, sep= ';', inferSchema = True)\n",
    "fname = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "df_temp = spark.read.csv(fname, header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "\n",
    "* The datasets were filtered by using the city. Port codes and cities were extracted from the labels of the immigrant event data. As city is common for all the datasets. each datset is cleaned for missing values, null values, duplicate values etc., Required columns were extracted for each table which provides more information in the schema design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#dealing I94port codes from I94 SAS labels\n",
    "ports=dict()\n",
    "with open(\"airportscode.txt\") as file:\n",
    "    for line in file:\n",
    "        (key, val) = line.split('=')\n",
    "        key = re.sub(\"[ '\\t]\", \"\",key)\n",
    "        val = (re.sub(\"['\\t\\n]\", \"\",val)).lower()\n",
    "        val= re.sub(r',[^,]*$', '', val)\n",
    "        ports[key] = val\n",
    "#defining user defined function to get cities for portcodes\n",
    "@udf\n",
    "def get_city(code):\n",
    "    for key,value in ports.items():\n",
    "        if code == key:\n",
    "            return ports[code]\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#performing cleaning and transformation on Data Immigration\n",
    "df_I94immigration = spark.read.parquet(\"sas_data\")\n",
    "df_I94immigration = df_I94immigration.filter(df_I94immigration.i94port.isin(list(ports.keys())))\n",
    "df_I94immigration = df_I94immigration.withColumn('arrdate', f.expr(\"date_add(to_date('1960-1-1'), arrdate)\"))\n",
    "df_I94immigration = df_I94immigration.withColumn('depdate', f.expr(\"date_add(to_date('1960-1-1'), depdate)\"))\n",
    "df_I94immigration = df_I94immigration.fillna({'i94mode':5})\n",
    "df_I94immigration = df_I94immigration.withColumn('i94City', get_city(df_I94immigration.i94port)).drop('i94port')\n",
    "df_I94immigration = df_I94immigration.select('cicid',\n",
    "                                              'i94yr',\n",
    "                                              'i94mon',\n",
    "                                              'arrdate',\n",
    "                                              'i94mode',\n",
    "                                              'depdate',\n",
    "                                              'i94bir',\n",
    "                                              'i94visa',\n",
    "                                              'biryear',\n",
    "                                              'visatype',\n",
    "                                             'i94City')\n",
    "df_I94immigration = df_I94immigration.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#cleanig and transforming airport data\n",
    "df_airport = df_airport.select('type','name','elevation_ft','municipality').dropDuplicates(['municipality'])\n",
    "df_airport = df_airport.withColumn('municipality',lower(col('municipality')))\n",
    "df_airport = df_airport.filter(df_airport.municipality.isin(list(ports.values())))\n",
    "df_airport = df_airport.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#cleanig and transforming usdemographics data\n",
    "df_usdemo = df_usdemo.select('City','State','Median Age','Total Population','Race').dropDuplicates(['City'])\n",
    "df_usdemo = df_usdemo.withColumn('City',lower(col('City')))\n",
    "df_usdemo = df_usdemo.filter(df_usdemo.City.isin(list(ports.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#transformation and cleaning temperature data\n",
    "df_temp = df_temp.select('AverageTemperature','City','Latitude','Longitude')\n",
    "df_temp = df_temp.withColumn('City', lower(col('City'))).dropDuplicates(['City'])\n",
    "df_temp = df_temp.filter(df_temp.City.isin(list(ports.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### Database Schema Design\n",
    "* Star schema is used to design the database as it is very simple and easy to understand relations between tables. A star schema contains one or more fact tables and several dimension tables connected to the fact table. The primary keys in dimension tables act as foreign keys in fact tables. In this project, the star schema contains one fact table and four dimension tables that are connected to the fact table. All four data sets are used in this schema.\n",
    "\n",
    "##### fact table\n",
    "* factTable\n",
    "##### dimension tables\n",
    "* df_I94immigration\n",
    "* df_temp\n",
    "* df_airport\n",
    "* df_usdemo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "\n",
    "The ETL pipeline is designed to load the datasets into the respective dimensional and fact tables with required columns \n",
    "\n",
    "* Loaded data into the respective spark dataframes\n",
    "* Filtered and Extracted required columns for the database design\n",
    "* Cleaned the data (like missing values, null values, etc)\n",
    "* Loaded the dimensional tables  into spark SQL tables with cleaned data from the above step \n",
    "* Created the fact table from the dimensional tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#creating dimension tables to extract fact table using sparksql\n",
    "df_I94immigration.createOrReplaceTempView('df_I94immigration')\n",
    "df_temp.createOrReplaceTempView('df_temp')\n",
    "df_airport.createOrReplaceTempView('df_airport')\n",
    "df_usdemo.createOrReplaceTempView('df_usdemo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#creating fact table \n",
    "factTable = sqlContext.sql(\n",
    "    \"\"\"SELECT a.cicid,a.i94yr,a.i94mon,a.arrdate,\n",
    "    a.i94mode,a.depdate,a.i94bir,a.i94visa,a.biryear,a.visatype,\n",
    "    a.i94City,b.AverageTemperature,b.Latitude,b.Longitude,c.State,'c.Median Age','c.Total Population',c.Race,\n",
    "    d.type,\n",
    "    d.name,\n",
    "    d.elevation_ft\n",
    "    FROM df_I94immigration as a\n",
    "    inner join df_temp as b on a.i94City == b.City\n",
    "    inner join df_usdemo as c on a.i94City == c.City\n",
    "    inner join df_airport as d on a.i94City == d.municipality\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# quality checks \n",
    "def quality_check(df,tablename):\n",
    "    '''\n",
    "    Input: Spark dataframe, table name\n",
    "    Output: Prints output of data quality check\n",
    "    '''\n",
    "    \n",
    "    rows = df.count()\n",
    "    if rows == 0:\n",
    "        print(\"Data quality check is failed for {} with zero records\".format(tablename))\n",
    "    else:\n",
    "        print(\"Data quality check is success for {} with {} records\".format(tablename, rows))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data quality check is success for immigration table with 2953808 records\n"
     ]
    }
   ],
   "source": [
    "quality_check(df_I94immigration, \"immigration table\")\n",
    "quality_check(factTable, \"fact table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# 4.3 Data dictionary \n",
    "## FactTable\n",
    "\n",
    "* cicid: unique identification of the immigrant\n",
    "* i94yr: 4 digit year\n",
    "* i94mon: month\n",
    "* arrdate: arrival date\n",
    "* i94mode: travel code\n",
    "* depdate: departure date\n",
    "* i94bir: age of immigrant\n",
    "* i94visa: visa code\n",
    "* biryear: date of birth\n",
    "* visatype: type of visa\n",
    "* i94City: city \n",
    "* AverageTemperature: city average temperature\n",
    "* Latitude: city latitude\n",
    "* Longitude, city longitude\n",
    "* State: city-state\n",
    "* Median Age': medium age in the state\n",
    "* Total Population': population in the city\n",
    "* Race: race \n",
    "* type: airport type\n",
    "* name: airport name in the city\n",
    "* elevation_ft: airport location elevation in feet\n",
    "\n",
    "## Dimension Tables\n",
    "\n",
    "### I94 immigration:\n",
    "* cicid: unique identification of the immigrant\n",
    "* i94yr: 4 digit year\n",
    "* i94mon: month\n",
    "* arrdate: arrival date\n",
    "* i94mode: travel code\n",
    "* depdate: departure date\n",
    "* i94bir: age of immigrant\n",
    "* i94visa: visa code\n",
    "* biryear: date of birth\n",
    "* visatype: type of visa\n",
    "* i94City: city \n",
    "\n",
    "### Temperature:\n",
    "* AverageTemperature = average temperature\n",
    "* City = city name\n",
    "* Latitude= latitude\n",
    "* Longitude = longitude\n",
    "\n",
    "### City Demographics \n",
    "* City: city\n",
    "* State: state\n",
    "* Median Age:  medium age in the state\n",
    "* Total Population: population in the city\n",
    "\n",
    "### Airports table\n",
    "* type: airport type\n",
    "* name: airport name in the city\n",
    "* elevation_ft: airport location elevation in feet\n",
    "* municipality: airport location\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Step 5: Complete Project Write Up\n",
    "#### Clearly state the rationale for the choice of tools and technologies for the project:\n",
    "* For this project, I used spark since it can handle process large amounts of data. Spark SQL is used to create fact and dimensional tables\n",
    "#### Propose how often the data should be updated and why:\n",
    "* The format of files is monthly we should load the data monthly\n",
    "\n",
    "## Write a description of how you would approach the problem differently under the following scenarios:\n",
    "#### the data was increased by 100x.\n",
    "* It is better to use the database with the ability to handle, process and read-heavy analytical  workloads on big data for example amazon redshift\n",
    "#### The data populates a dashboard that must be updated on a daily basis by 7 am every day.\n",
    "* Use a scheduling framework to schedule the tasks on daily basis \n",
    "* For example, use Airflow, create DAG retries, or send emails on failures.\n",
    "#### The database needed to be accessed by 100+ people.\n",
    "* Using more nodes and distributed data storage to decrease the load on the database so that database can access by more people.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
